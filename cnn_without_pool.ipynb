{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and reshape accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "print train_data.shape #(42000, 784)\n",
    "print test_data.shape #(28000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.drop(['label'], axis = 1)\n",
    "label = train_data.label\n",
    "target = pd.get_dummies(label, columns=['label'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 784)\n",
      "(10500, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_cv,y_train,y_cv = train_test_split(train,target,test_size = 0.25, random_state = 4)\n",
    "print x_train.shape #(31500, 784)\n",
    "print x_cv.shape #(10500, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape df\n",
    "x_arr = np.array(x_train)\n",
    "x_cv_arr = np.array(x_cv)\n",
    "X = x_arr.reshape(31500,28,28,1)\n",
    "X_cv = x_cv_arr.reshape(10500,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing weights and bias for convolution layer\n",
    "\n",
    "W1 = 0.1*np.random.rand(3,3,3,1)\n",
    "b1 = 0.1*np.random.rand(3,1)\n",
    "\n",
    "## Initializing weights and bias for fully connected layer\n",
    "theta = 0.1*np.random.rand(2352,10)\n",
    "bias = 0.1*np.random.rand(1,10)\n",
    "\n",
    "## Normalizing input data\n",
    "x_arr -= int(np.mean(x_arr))\n",
    "x_arr = x_arr.astype(float)\n",
    "x_arr /= int(np.std(x_arr))\n",
    "\n",
    "## Stacking features and labels \n",
    "train_data = np.hstack((x_arr,np.array(y_train)))\n",
    "t = train_data[0:400]\n",
    "\n",
    "## Normalizing cross-validation data\n",
    "x_cv_arr -= int(np.mean(x_cv_arr))\n",
    "x_cv_arr = x_cv_arr.astype(float)\n",
    "x_cv_arr /= int(np.std(x_cv_arr))\n",
    "\n",
    "## Training the model on 400 images, and cv on 100 due to computation issue\n",
    "cv_data = np.hstack((x_cv_arr,np.array(y_cv)))\n",
    "test_data = x_cv_arr[0:100]\n",
    "Y_cv = np.array(y_cv)[0:100]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "## Assigning hyperparameter values\n",
    "learning_rate = 0.01\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "num_images = len(t)   ##Number of the input training examples\n",
    "w = 28\n",
    "l = 1\n",
    "l1 = len(W1)    ## no. of filters in W1\n",
    "f = len(W1[0])\n",
    "\n",
    "# print X_cv.shape (10500, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PADDING function\n",
    "def zero_pad(data, pad):\n",
    "    data_pad = np.pad(data,((0,0),(pad,pad),(pad,pad),(0,0)), 'constant')\n",
    "    return data_pad\n",
    "\n",
    "## Softmax\n",
    "def softmax_cost(out,y):\n",
    "    eout = np.exp(out, dtype=np.float)  \n",
    "    probs = eout/np.sum(eout, axis = 1)[:,None]\n",
    "    \n",
    "    p = np.sum(np.multiply(y,probs), axis = 1)\n",
    "    prob_label = np.argmax(np.array(probs), axis = 1)    # arguments of max values\n",
    "    cost = -np.log(p)    # -log(y*prob)\n",
    "    \n",
    "    return p, cost, probs, prob_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(input_data, Y, W1, b1, theta, bias):\n",
    "    \n",
    "## Forward propagation\n",
    "\n",
    "    ## Input shape\n",
    "    m, w, w, c = input_data.shape\n",
    "    \n",
    "    ## no. of filters in layer_1\n",
    "    l1 = len(W1)\n",
    "\n",
    "    ## Shape of the filter used\n",
    "    (f, f, _) = W1[0].shape\n",
    "    pad = 1\n",
    "    ## stride = 1, to make calculations easier\n",
    "    \n",
    "    ## Convolution layer1 output dimensions\n",
    "    nw1 = w+(2*pad)-f + 1\n",
    "\n",
    "    ## Initializing output image matrices after convolutions\n",
    "    conv1 = np.zeros((m,nw1,nw1,l1))\n",
    "\n",
    "    ## Padding the input images\n",
    "    input_pad = zero_pad(input_data,pad)\n",
    "\n",
    "    ## Convolution layer\n",
    "    ## Looping over the no. of examples, no. of filters, height and width (h,w) of image\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l1):\n",
    "            for k in range(0,nw1): \n",
    "                for l in range(0,nw1):\n",
    "                    conv1[i,k,l,j] = np.sum(input_pad[i,k:k+f,l:l+f]*W1[j])+b1[j]\n",
    "\n",
    "        conv1[i,:,:,:][conv1[i,:,:,:] <= 0] = 0                           ##relu activation\n",
    "    #print \"conv1\",conv1.shape\n",
    "\n",
    "    ## Fully connected layer of neurons\n",
    "    fc1 = conv1.reshape(m,int((nw1)*(nw1)*l1))\n",
    "    #print \"fc1\", fc1.shape\n",
    "    \n",
    "    ## Output layer of mx10 activation units\n",
    "    out = np.dot(fc1,theta) + bias\n",
    "        \n",
    "    ## Using softmax to get the cost    \n",
    "    p, cost, probs, prob_label = softmax_cost(out, Y)\n",
    "    \n",
    "    acc = []\n",
    "    for i in range(0,len(Y)):\n",
    "        if prob_label[i]==np.argmax(np.array(Y)[i,:]):\n",
    "            acc.append(1)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "\n",
    "## Backpropagation to calculate gradients \n",
    "    \n",
    "    #Backpropogation across loss and softmax\n",
    "    d_out = probs - Y\n",
    "    #print \"d_out\", d_out.shape\n",
    "    #Fully connected layer\n",
    "    dtheta = np.dot(d_out.T, fc1)\n",
    "    dbias = np.mean(d_out, axis = 0).reshape(1,10)    \n",
    "\n",
    "    dfc1 = np.dot(theta,d_out.T)\n",
    "    #print \"dfc1\",dfc1.shape\n",
    "    #Pooling and Convolution layer\n",
    "    #dpool = dfc1.T.reshape((m, int(nw1/2), int(nw1/2), l1))\n",
    "    dconv1 = dfc1.T.reshape((m, nw1, nw1, l1)) #initialization \n",
    "    #print \"dconv1\", dconv1.shape\n",
    "\n",
    "    dconv1[conv1<=0]=0 #brelu\n",
    "\n",
    "    \n",
    "    dW1_stack = np.zeros((m,l1,f,f,1))\n",
    "    db1_stack = np.zeros((m,l1,1))\n",
    "\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "\n",
    "    ## looping through the one batch of 40 examples\n",
    "    for i in range(0,m):\n",
    "        for c in range(0,l1):\n",
    "            for x in range(0,nw1):\n",
    "                for y in range(0,nw1):\n",
    "                    dW1_stack[i,:,:,c] += dconv1[i,x,y,c]*input_pad[i,x:x+f,y:y+f,:]\n",
    "            db1_stack[i,c] = np.sum(dconv1[i,:,:,c])\n",
    "        dconv1[conv1<=0]=0\n",
    "        \n",
    "        dW1 = np.mean(dW1_stack, axis = 0)\n",
    "        db1 = np.mean(db1_stack, axis = 0)\n",
    "        \n",
    "    return dW1, db1, dtheta, dbias, cost, probs, prob_label, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(batch,learning_rate,W1,b1,theta,bias):\n",
    "    \n",
    "    ## Slicing train data and labels from batch\n",
    "    X = batch[:,0:-10]\n",
    "    X = X.reshape(len(batch), w, w, l)\n",
    "    Y = batch[:,784:794]\n",
    "    \n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    ## Initializing gradient matrices \n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    dtheta = np.zeros(theta.shape)\n",
    "    dbias = np.zeros(bias.shape)\n",
    "    \n",
    "    grads = conv_net(X,Y,W1,b1,theta,bias)\n",
    "    [dW1, db1, dtheta, dbias, cost_, probs_, prob_label, acc_] = grads\n",
    "    \n",
    "    #Updating weights for convolution layer and biases\n",
    "    W1 = W1-learning_rate*(dW1) #convolution\n",
    "    b1 = b1-learning_rate*(db1)\n",
    "    theta = theta-learning_rate*(dtheta.T) #fully connected layer\n",
    "    bias = bias-learning_rate*(dbias)\n",
    "    \n",
    "    batch_cost = np.mean(cost_) # calculating the cost for each batch\n",
    "    batch_accuracy = sum(acc_)/len(acc_) #Reporting the accuracy for each batch\n",
    "    \n",
    "    return W1, b1, theta, bias, batch_cost, acc_, batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_init(train_data,W1,b1,theta,bias):\n",
    "    \n",
    "    cost = []\n",
    "    accuracy = []\n",
    "    for epoch in range(0, num_epochs):\n",
    "        batches = [train_data[k:k + batch_size] for k in xrange(0, len(train_data), batch_size)]\n",
    "        x=0\n",
    "        i = 1\n",
    "        for batch in batches:\n",
    "            \n",
    "            output = optimizer(batch,learning_rate,W1,b1,theta,bias)\n",
    "            [W1, b1, theta, bias, batch_cost,acc_,batch_acc] = output\n",
    "                        \n",
    "            cost.append(batch_cost)\n",
    "            accuracy.append(batch_acc)\n",
    "\n",
    "            print 'ep:%d, Batch_no = %d, Cost = %f, Accuracy = %.2f %%' %(epoch,i,batch_cost,batch_acc*100) \n",
    "            i+=1\n",
    "        print '\\nAfter epoch %d, Batch Cost = %f, Batch Accuracy = %.2f %%\\n' %(epoch,batch_cost,batch_acc*100)\n",
    "    return W1,b1,theta,bias,cost,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:0, Batch_no = 1, Cost = 2.402001, Accuracy = 7.50 %\n",
      "ep:0, Batch_no = 2, Cost = 4.822523, Accuracy = 17.50 %\n",
      "ep:0, Batch_no = 3, Cost = 3.219694, Accuracy = 42.50 %\n",
      "ep:0, Batch_no = 4, Cost = 2.342101, Accuracy = 12.50 %\n",
      "ep:0, Batch_no = 5, Cost = 2.259545, Accuracy = 35.00 %\n",
      "ep:0, Batch_no = 6, Cost = 2.112560, Accuracy = 42.50 %\n",
      "ep:0, Batch_no = 7, Cost = 1.979952, Accuracy = 42.50 %\n",
      "ep:0, Batch_no = 8, Cost = 2.173199, Accuracy = 52.50 %\n",
      "ep:0, Batch_no = 9, Cost = 1.960518, Accuracy = 60.00 %\n",
      "ep:0, Batch_no = 10, Cost = 2.218523, Accuracy = 57.50 %\n",
      "\n",
      "After epoch 0, Batch Cost = 2.218523, Batch Accuracy = 57.50 %\n",
      "\n",
      "ep:1, Batch_no = 1, Cost = 1.956877, Accuracy = 70.00 %\n",
      "ep:1, Batch_no = 2, Cost = 1.475618, Accuracy = 70.00 %\n",
      "ep:1, Batch_no = 3, Cost = 1.388756, Accuracy = 57.50 %\n",
      "ep:1, Batch_no = 4, Cost = 1.179050, Accuracy = 70.00 %\n",
      "ep:1, Batch_no = 5, Cost = 1.086756, Accuracy = 62.50 %\n",
      "ep:1, Batch_no = 6, Cost = 1.618678, Accuracy = 42.50 %\n",
      "ep:1, Batch_no = 7, Cost = 1.700454, Accuracy = 57.50 %\n",
      "ep:1, Batch_no = 8, Cost = 1.362430, Accuracy = 60.00 %\n",
      "ep:1, Batch_no = 9, Cost = 0.814719, Accuracy = 72.50 %\n",
      "ep:1, Batch_no = 10, Cost = 0.665489, Accuracy = 75.00 %\n",
      "\n",
      "After epoch 1, Batch Cost = 0.665489, Batch Accuracy = 75.00 %\n",
      "\n",
      "ep:2, Batch_no = 1, Cost = 0.330348, Accuracy = 92.50 %\n",
      "ep:2, Batch_no = 2, Cost = 0.436630, Accuracy = 90.00 %\n",
      "ep:2, Batch_no = 3, Cost = 0.856830, Accuracy = 60.00 %\n",
      "ep:2, Batch_no = 4, Cost = 0.763388, Accuracy = 85.00 %\n",
      "ep:2, Batch_no = 5, Cost = 0.679652, Accuracy = 67.50 %\n",
      "ep:2, Batch_no = 6, Cost = 1.271578, Accuracy = 60.00 %\n",
      "ep:2, Batch_no = 7, Cost = 1.457157, Accuracy = 65.00 %\n",
      "ep:2, Batch_no = 8, Cost = 1.197093, Accuracy = 67.50 %\n",
      "ep:2, Batch_no = 9, Cost = 0.580808, Accuracy = 77.50 %\n",
      "ep:2, Batch_no = 10, Cost = 0.484800, Accuracy = 85.00 %\n",
      "\n",
      "After epoch 2, Batch Cost = 0.484800, Batch Accuracy = 85.00 %\n",
      "\n",
      "ep:3, Batch_no = 1, Cost = 0.193266, Accuracy = 100.00 %\n",
      "ep:3, Batch_no = 2, Cost = 0.396446, Accuracy = 90.00 %\n",
      "ep:3, Batch_no = 3, Cost = 0.659204, Accuracy = 80.00 %\n",
      "ep:3, Batch_no = 4, Cost = 0.413236, Accuracy = 87.50 %\n",
      "ep:3, Batch_no = 5, Cost = 0.481258, Accuracy = 85.00 %\n",
      "ep:3, Batch_no = 6, Cost = 1.268382, Accuracy = 57.50 %\n",
      "ep:3, Batch_no = 7, Cost = 1.894608, Accuracy = 70.00 %\n",
      "ep:3, Batch_no = 8, Cost = 1.033788, Accuracy = 65.00 %\n",
      "ep:3, Batch_no = 9, Cost = 0.615707, Accuracy = 77.50 %\n",
      "ep:3, Batch_no = 10, Cost = 0.474643, Accuracy = 85.00 %\n",
      "\n",
      "After epoch 3, Batch Cost = 0.474643, Batch Accuracy = 85.00 %\n",
      "\n",
      "ep:4, Batch_no = 1, Cost = 0.150323, Accuracy = 97.50 %\n",
      "ep:4, Batch_no = 2, Cost = 0.347645, Accuracy = 92.50 %\n",
      "ep:4, Batch_no = 3, Cost = 0.481762, Accuracy = 85.00 %\n",
      "ep:4, Batch_no = 4, Cost = 0.316531, Accuracy = 92.50 %\n",
      "ep:4, Batch_no = 5, Cost = 0.373150, Accuracy = 90.00 %\n",
      "ep:4, Batch_no = 6, Cost = 0.666185, Accuracy = 75.00 %\n",
      "ep:4, Batch_no = 7, Cost = 0.947753, Accuracy = 77.50 %\n",
      "ep:4, Batch_no = 8, Cost = 0.676802, Accuracy = 77.50 %\n",
      "ep:4, Batch_no = 9, Cost = 0.428335, Accuracy = 80.00 %\n",
      "ep:4, Batch_no = 10, Cost = 0.270608, Accuracy = 92.50 %\n",
      "\n",
      "After epoch 4, Batch Cost = 0.270608, Batch Accuracy = 92.50 %\n",
      "\n",
      "ep:5, Batch_no = 1, Cost = 0.083932, Accuracy = 100.00 %\n",
      "ep:5, Batch_no = 2, Cost = 0.303989, Accuracy = 92.50 %\n",
      "ep:5, Batch_no = 3, Cost = 0.440238, Accuracy = 85.00 %\n",
      "ep:5, Batch_no = 4, Cost = 0.325436, Accuracy = 92.50 %\n",
      "ep:5, Batch_no = 5, Cost = 0.278148, Accuracy = 95.00 %\n",
      "ep:5, Batch_no = 6, Cost = 0.470434, Accuracy = 80.00 %\n",
      "ep:5, Batch_no = 7, Cost = 0.664336, Accuracy = 82.50 %\n",
      "ep:5, Batch_no = 8, Cost = 0.556348, Accuracy = 85.00 %\n",
      "ep:5, Batch_no = 9, Cost = 0.336562, Accuracy = 85.00 %\n",
      "ep:5, Batch_no = 10, Cost = 0.207166, Accuracy = 95.00 %\n",
      "\n",
      "After epoch 5, Batch Cost = 0.207166, Batch Accuracy = 95.00 %\n",
      "\n",
      "ep:6, Batch_no = 1, Cost = 0.063850, Accuracy = 100.00 %\n",
      "ep:6, Batch_no = 2, Cost = 0.271140, Accuracy = 92.50 %\n",
      "ep:6, Batch_no = 3, Cost = 0.368953, Accuracy = 87.50 %\n",
      "ep:6, Batch_no = 4, Cost = 0.258852, Accuracy = 92.50 %\n",
      "ep:6, Batch_no = 5, Cost = 0.243991, Accuracy = 95.00 %\n",
      "ep:6, Batch_no = 6, Cost = 0.424690, Accuracy = 82.50 %\n",
      "ep:6, Batch_no = 7, Cost = 0.615985, Accuracy = 82.50 %\n",
      "ep:6, Batch_no = 8, Cost = 0.491205, Accuracy = 82.50 %\n",
      "ep:6, Batch_no = 9, Cost = 0.278925, Accuracy = 90.00 %\n",
      "ep:6, Batch_no = 10, Cost = 0.197370, Accuracy = 90.00 %\n",
      "\n",
      "After epoch 6, Batch Cost = 0.197370, Batch Accuracy = 90.00 %\n",
      "\n",
      "ep:7, Batch_no = 1, Cost = 0.057916, Accuracy = 100.00 %\n",
      "ep:7, Batch_no = 2, Cost = 0.251580, Accuracy = 92.50 %\n",
      "ep:7, Batch_no = 3, Cost = 0.301385, Accuracy = 87.50 %\n",
      "ep:7, Batch_no = 4, Cost = 0.217952, Accuracy = 92.50 %\n",
      "ep:7, Batch_no = 5, Cost = 0.199697, Accuracy = 95.00 %\n",
      "ep:7, Batch_no = 6, Cost = 0.274567, Accuracy = 90.00 %\n",
      "ep:7, Batch_no = 7, Cost = 0.329798, Accuracy = 90.00 %\n",
      "ep:7, Batch_no = 8, Cost = 0.330943, Accuracy = 92.50 %\n",
      "ep:7, Batch_no = 9, Cost = 0.218058, Accuracy = 90.00 %\n",
      "ep:7, Batch_no = 10, Cost = 0.164854, Accuracy = 95.00 %\n",
      "\n",
      "After epoch 7, Batch Cost = 0.164854, Batch Accuracy = 95.00 %\n",
      "\n",
      "ep:8, Batch_no = 1, Cost = 0.049833, Accuracy = 100.00 %\n",
      "ep:8, Batch_no = 2, Cost = 0.186478, Accuracy = 92.50 %\n",
      "ep:8, Batch_no = 3, Cost = 0.230601, Accuracy = 87.50 %\n",
      "ep:8, Batch_no = 4, Cost = 0.243580, Accuracy = 95.00 %\n",
      "ep:8, Batch_no = 5, Cost = 0.152598, Accuracy = 95.00 %\n",
      "ep:8, Batch_no = 6, Cost = 0.190232, Accuracy = 97.50 %\n",
      "ep:8, Batch_no = 7, Cost = 0.223575, Accuracy = 90.00 %\n",
      "ep:8, Batch_no = 8, Cost = 0.238638, Accuracy = 92.50 %\n",
      "ep:8, Batch_no = 9, Cost = 0.186311, Accuracy = 92.50 %\n",
      "ep:8, Batch_no = 10, Cost = 0.098570, Accuracy = 100.00 %\n",
      "\n",
      "After epoch 8, Batch Cost = 0.098570, Batch Accuracy = 100.00 %\n",
      "\n",
      "ep:9, Batch_no = 1, Cost = 0.030584, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 2, Cost = 0.166642, Accuracy = 97.50 %\n",
      "ep:9, Batch_no = 3, Cost = 0.236519, Accuracy = 90.00 %\n",
      "ep:9, Batch_no = 4, Cost = 0.159031, Accuracy = 92.50 %\n",
      "ep:9, Batch_no = 5, Cost = 0.120570, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 6, Cost = 0.127928, Accuracy = 97.50 %\n",
      "ep:9, Batch_no = 7, Cost = 0.172821, Accuracy = 95.00 %\n",
      "ep:9, Batch_no = 8, Cost = 0.194772, Accuracy = 95.00 %\n",
      "ep:9, Batch_no = 9, Cost = 0.112747, Accuracy = 95.00 %\n",
      "ep:9, Batch_no = 10, Cost = 0.056482, Accuracy = 100.00 %\n",
      "\n",
      "After epoch 9, Batch Cost = 0.056482, Batch Accuracy = 100.00 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W1_t,b1_t,theta_t,bias_t,cost_t,accuracy_t = main_init(t,W1,b1,theta,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
