{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and reshape accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "print train_data.shape #(42000, 784)\n",
    "print test_data.shape #(28000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.drop(['label'], axis = 1)\n",
    "label = train_data.label\n",
    "target = pd.get_dummies(label, columns=['label'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 784)\n",
      "(10500, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_cv,y_train,y_cv = train_test_split(train,target,test_size = 0.25, random_state = 4)\n",
    "print x_train.shape #(31500, 784)\n",
    "print x_cv.shape #(10500, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape df\n",
    "x_arr = np.array(x_train)\n",
    "x_cv_arr = np.array(x_cv)\n",
    "X = x_arr.reshape(31500,28,28,1)\n",
    "X_cv = x_cv_arr.reshape(10500,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing weights and bias for convolution layer\n",
    "\n",
    "W1 = 0.1*np.random.rand(3,3,3,1)\n",
    "b1 = 0.1*np.random.rand(3,1)\n",
    "\n",
    "## Initializing weights and bias for fully connected layer\n",
    "theta = 0.1*np.random.rand(2352,10)\n",
    "bias = 0.1*np.random.rand(1,10)\n",
    "\n",
    "## Normalizing input data\n",
    "x_arr -= int(np.mean(x_arr))\n",
    "x_arr = x_arr.astype(float)\n",
    "x_arr /= int(np.std(x_arr))\n",
    "\n",
    "## Stacking features and labels \n",
    "train_data = np.hstack((x_arr,np.array(y_train)))\n",
    "t = train_data[0:400]\n",
    "\n",
    "## Normalizing cross-validation data\n",
    "x_cv_arr -= int(np.mean(x_cv_arr))\n",
    "x_cv_arr = x_cv_arr.astype(float)\n",
    "x_cv_arr /= int(np.std(x_cv_arr))\n",
    "\n",
    "## Training the model on 400 images, and cv on 100 due to computation issue\n",
    "cv_data = np.hstack((x_cv_arr,np.array(y_cv)))\n",
    "test_data = x_cv_arr[0:100]\n",
    "Y_cv = np.array(y_cv)[0:100]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "## Assigning hyperparameter values\n",
    "learning_rate = 0.01\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "num_images = len(t)   ##Number of the input training examples\n",
    "w = 28\n",
    "l = 1\n",
    "l1 = len(W1)    ## no. of filters in W1\n",
    "f = len(W1[0])\n",
    "\n",
    "# print X_cv.shape (10500, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PADDING function\n",
    "def zero_pad(data, pad):\n",
    "    data_pad = np.pad(data,((0,0),(pad,pad),(pad,pad),(0,0)), 'constant')\n",
    "    return data_pad\n",
    "\n",
    "## Function to get the coordinates of maxpool element\n",
    "def idxargmax(a):\n",
    "    idx = np.argmax(a, axis=None)\n",
    "    multi_idx = np.unravel_index(idx, a.shape)\n",
    "    if np.isnan(a[multi_idx]):\n",
    "        nan_count = np.sum(np.isnan(a))\n",
    "        idx = np.argpartition(a, -nan_count-1, axis=None)[-nan_count-1]\n",
    "        idx = np.argsort(a, axis=None)[-nan_count-1]\n",
    "        multi_idx = np.unravel_index(idx, a.shape)\n",
    "    return multi_idx\n",
    "\n",
    "## Maxpool function\n",
    "def max_pool(X,f,stride):\n",
    "    (m, w, w, c) = X.shape\n",
    "    output_size = int((w-f)/stride+1)\n",
    "    pool = np.zeros((m,output_size,output_size, c))\n",
    "    for e in range(0,m):\n",
    "        for k in range(0,c):\n",
    "            for i in range(output_size):\n",
    "                for j in range(output_size):\n",
    "                    pool[e,i,j,k] = np.max(X[e,i*stride:i*stride+f,j*stride:j*stride+f,k])\n",
    "    return pool\n",
    "\n",
    "## Softmax\n",
    "def softmax_cost(out,y):\n",
    "    eout = np.exp(out, dtype=np.float)  \n",
    "    probs = eout/np.sum(eout, axis = 1)[:,None]\n",
    "    \n",
    "    p = np.sum(np.multiply(y,probs), axis = 1)\n",
    "    prob_label = np.argmax(np.array(probs), axis = 1)    # arguments of max values\n",
    "    cost = -np.log(p)    # -log(y*prob)\n",
    "    \n",
    "    return p, cost, probs, prob_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(input_data, Y, W1, b1, theta, bias):\n",
    "    \n",
    "## Forward propagation\n",
    "\n",
    "    ## Input shape\n",
    "    m, w, w, c = input_data.shape\n",
    "    \n",
    "    ## no. of filters in layer_1\n",
    "    l1 = len(W1)\n",
    "\n",
    "    ## Shape of the filter used\n",
    "    (f, f, _) = W1[0].shape\n",
    "    pad = 1\n",
    "    ## stride = 1, to make calculations easier\n",
    "    \n",
    "    ## Convolution layer1 output dimensions\n",
    "    nw1 = w+(2*pad)-f + 1\n",
    "\n",
    "    ## Initializing output image matrices after convolutions\n",
    "    conv1 = np.zeros((m,nw1,nw1,l1))\n",
    "\n",
    "    ## Padding the input images\n",
    "    input_pad = zero_pad(input_data,pad)\n",
    "\n",
    "    ## Convolution layer\n",
    "    ## Looping over the no. of examples, no. of filters, height and width (h,w) of image\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l1):\n",
    "            for k in range(0,nw1): \n",
    "                for l in range(0,nw1):\n",
    "                    conv1[i,k,l,j] = np.sum(input_pad[i,k:k+f,l:l+f]*W1[j])+b1[j]\n",
    "\n",
    "        conv1[i,:,:,:][conv1[i,:,:,:] <= 0] = 0                           ##relu activation\n",
    "    #print \"conv1\",conv1.shape\n",
    "    '''\n",
    "    ## Pooling layer after max_pooling filter size of 2x2 and stride 2\n",
    "    pooled_layer = max_pool(conv1, 2, 2)  \n",
    "    '''\n",
    "    ## Fully connected layer of neurons\n",
    "    fc1 = conv1.reshape(m,int((nw1)*(nw1)*l1))\n",
    "    #print \"fc1\", fc1.shape\n",
    "    \n",
    "    ## Output layer of mx10 activation units\n",
    "    out = np.dot(fc1,theta) + bias\n",
    "        \n",
    "    ## Using softmax to get the cost    \n",
    "    p, cost, probs, prob_label = softmax_cost(out, Y)\n",
    "    \n",
    "    acc = []\n",
    "    for i in range(0,len(Y)):\n",
    "        if prob_label[i]==np.argmax(np.array(Y)[i,:]):\n",
    "            acc.append(1)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "\n",
    "## Backpropagation to calculate gradients \n",
    "    \n",
    "    #Backpropogation across loss and softmax\n",
    "    d_out = probs - Y\n",
    "    #print \"d_out\", d_out.shape\n",
    "    #Fully connected layer\n",
    "    dtheta = np.dot(d_out.T, fc1)\n",
    "    dbias = np.mean(d_out, axis = 0).reshape(1,10)    \n",
    "\n",
    "    dfc1 = np.dot(theta,d_out.T)\n",
    "    #print \"dfc1\",dfc1.shape\n",
    "    #Pooling and Convolution layer\n",
    "    #dpool = dfc1.T.reshape((m, int(nw1/2), int(nw1/2), l1))\n",
    "    dconv1 = dfc1.T.reshape((m, nw1, nw1, l1)) #initialization \n",
    "    #print \"dconv1\", dconv1.shape\n",
    "    '''\n",
    "    for k in range(0,m):\n",
    "        for c in range(0,l1):\n",
    "            i=0\n",
    "            while(i<nw1):\n",
    "                j=0\n",
    "                while(j<nw1):\n",
    "                    (a,b) = idxargmax(conv1[k,i:i+2,j:j+2,c]) ## Getting indexes of maximum value in the array\n",
    "                    dconv1[k,i+a,j+b,c] = dpool[k,int(i/2),int(j/2),c]\n",
    "                    j+=2\n",
    "                i+=2\n",
    "    '''\n",
    "    dconv1[conv1<=0]=0 #brelu\n",
    "\n",
    "    \n",
    "    dW1_stack = np.zeros((m,l1,f,f,1))\n",
    "    db1_stack = np.zeros((m,l1,1))\n",
    "\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "\n",
    "    ## looping through the one batch of 40 examples\n",
    "    for i in range(0,m):\n",
    "        for c in range(0,l1):\n",
    "            for x in range(0,nw1):\n",
    "                for y in range(0,nw1):\n",
    "                    dW1_stack[i,:,:,c] += dconv1[i,x,y,c]*input_pad[i,x:x+f,y:y+f,:]\n",
    "            db1_stack[i,c] = np.sum(dconv1[i,:,:,c])\n",
    "        dconv1[conv1<=0]=0\n",
    "        \n",
    "        dW1 = np.mean(dW1_stack, axis = 0)\n",
    "        db1 = np.mean(db1_stack, axis = 0)\n",
    "        \n",
    "    return dW1, db1, dtheta, dbias, cost, probs, prob_label, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(batch,learning_rate,W1,b1,theta,bias):\n",
    "    \n",
    "    ## Slicing train data and labels from batch\n",
    "    X = batch[:,0:-10]\n",
    "    X = X.reshape(len(batch), w, w, l)\n",
    "    Y = batch[:,784:794]\n",
    "    \n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    ## Initializing gradient matrices \n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    dtheta = np.zeros(theta.shape)\n",
    "    dbias = np.zeros(bias.shape)\n",
    "    \n",
    "    grads = conv_net(X,Y,W1,b1,theta,bias)\n",
    "    [dW1, db1, dtheta, dbias, cost_, probs_, prob_label, acc_] = grads\n",
    "    \n",
    "    #Updating weights for convolution layer and biases\n",
    "    W1 = W1-learning_rate*(dW1) #convolution\n",
    "    b1 = b1-learning_rate*(db1)\n",
    "    theta = theta-learning_rate*(dtheta.T) #fully connected layer\n",
    "    bias = bias-learning_rate*(dbias)\n",
    "    \n",
    "    batch_cost = np.mean(cost_) # calculating the cost for each batch\n",
    "    batch_accuracy = sum(acc_)/len(acc_) #Reporting the accuracy for each batch\n",
    "    \n",
    "    return W1, b1, theta, bias, batch_cost, acc_, batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_init(train_data,W1,b1,theta,bias):\n",
    "    \n",
    "    cost = []\n",
    "    accuracy = []\n",
    "    for epoch in range(0, num_epochs):\n",
    "        batches = [train_data[k:k + batch_size] for k in xrange(0, len(train_data), batch_size)]\n",
    "        x=0\n",
    "        i = 1\n",
    "        for batch in batches:\n",
    "            \n",
    "            output = optimizer(batch,learning_rate,W1,b1,theta,bias)\n",
    "            [W1, b1, theta, bias, batch_cost,acc_,batch_acc] = output\n",
    "                        \n",
    "            cost.append(batch_cost)\n",
    "            accuracy.append(batch_acc)\n",
    "\n",
    "            print 'ep:%d, Batch_no = %f, Cost = %f, Accuracy = %.2f %%' %(epoch,i,batch_cost,batch_acc*100) \n",
    "            i+=1\n",
    "        print '\\nAfter epoch %d, Batch Cost = %f, Batch Accuracy = %.2f %%\\n' %(epoch,batch_cost,batch_acc*100)\n",
    "    return W1,b1,theta,bias,cost,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:0, Batch_no = 1.000000, Cost = 2.541080, Accuracy = 7.50 %\n",
      "ep:0, Batch_no = 2.000000, Cost = 8.041162, Accuracy = 5.00 %\n",
      "ep:0, Batch_no = 3.000000, Cost = 2.946771, Accuracy = 42.50 %\n",
      "ep:0, Batch_no = 4.000000, Cost = 2.259020, Accuracy = 7.50 %\n",
      "ep:0, Batch_no = 5.000000, Cost = 2.552195, Accuracy = 32.50 %\n",
      "ep:0, Batch_no = 6.000000, Cost = 2.207576, Accuracy = 30.00 %\n",
      "ep:0, Batch_no = 7.000000, Cost = 2.168430, Accuracy = 40.00 %\n",
      "ep:0, Batch_no = 8.000000, Cost = 2.434300, Accuracy = 32.50 %\n",
      "ep:0, Batch_no = 9.000000, Cost = 2.162686, Accuracy = 32.50 %\n",
      "ep:0, Batch_no = 10.000000, Cost = 2.205329, Accuracy = 37.50 %\n",
      "\n",
      "After epoch 0, Batch Cost = 2.205329, Batch Accuracy = 37.50 %\n",
      "\n",
      "ep:1, Batch_no = 1.000000, Cost = 2.360860, Accuracy = 42.50 %\n",
      "ep:1, Batch_no = 2.000000, Cost = 1.878623, Accuracy = 62.50 %\n",
      "ep:1, Batch_no = 3.000000, Cost = 1.936615, Accuracy = 60.00 %\n",
      "ep:1, Batch_no = 4.000000, Cost = 1.704222, Accuracy = 55.00 %\n",
      "ep:1, Batch_no = 5.000000, Cost = 2.355322, Accuracy = 57.50 %\n",
      "ep:1, Batch_no = 6.000000, Cost = 2.010853, Accuracy = 42.50 %\n",
      "ep:1, Batch_no = 7.000000, Cost = 1.867178, Accuracy = 45.00 %\n",
      "ep:1, Batch_no = 8.000000, Cost = 2.109924, Accuracy = 37.50 %\n",
      "ep:1, Batch_no = 9.000000, Cost = 1.742978, Accuracy = 42.50 %\n",
      "ep:1, Batch_no = 10.000000, Cost = 1.896268, Accuracy = 42.50 %\n",
      "\n",
      "After epoch 1, Batch Cost = 1.896268, Batch Accuracy = 42.50 %\n",
      "\n",
      "ep:2, Batch_no = 1.000000, Cost = 2.117931, Accuracy = 52.50 %\n",
      "ep:2, Batch_no = 2.000000, Cost = 1.258673, Accuracy = 77.50 %\n",
      "ep:2, Batch_no = 3.000000, Cost = 1.441893, Accuracy = 75.00 %\n",
      "ep:2, Batch_no = 4.000000, Cost = 1.304844, Accuracy = 57.50 %\n",
      "ep:2, Batch_no = 5.000000, Cost = 1.751780, Accuracy = 57.50 %\n",
      "ep:2, Batch_no = 6.000000, Cost = 1.106013, Accuracy = 57.50 %\n",
      "ep:2, Batch_no = 7.000000, Cost = 0.801864, Accuracy = 67.50 %\n",
      "ep:2, Batch_no = 8.000000, Cost = 0.979913, Accuracy = 67.50 %\n",
      "ep:2, Batch_no = 9.000000, Cost = 1.174440, Accuracy = 55.00 %\n",
      "ep:2, Batch_no = 10.000000, Cost = 1.147026, Accuracy = 70.00 %\n",
      "\n",
      "After epoch 2, Batch Cost = 1.147026, Batch Accuracy = 70.00 %\n",
      "\n",
      "ep:3, Batch_no = 1.000000, Cost = 0.912648, Accuracy = 72.50 %\n",
      "ep:3, Batch_no = 2.000000, Cost = 0.744528, Accuracy = 80.00 %\n",
      "ep:3, Batch_no = 3.000000, Cost = 0.564153, Accuracy = 82.50 %\n",
      "ep:3, Batch_no = 4.000000, Cost = 0.703937, Accuracy = 72.50 %\n",
      "ep:3, Batch_no = 5.000000, Cost = 0.391764, Accuracy = 82.50 %\n",
      "ep:3, Batch_no = 6.000000, Cost = 0.451210, Accuracy = 80.00 %\n",
      "ep:3, Batch_no = 7.000000, Cost = 0.423583, Accuracy = 87.50 %\n",
      "ep:3, Batch_no = 8.000000, Cost = 0.575210, Accuracy = 80.00 %\n",
      "ep:3, Batch_no = 9.000000, Cost = 0.597384, Accuracy = 82.50 %\n",
      "ep:3, Batch_no = 10.000000, Cost = 0.474685, Accuracy = 85.00 %\n",
      "\n",
      "After epoch 3, Batch Cost = 0.474685, Batch Accuracy = 85.00 %\n",
      "\n",
      "ep:4, Batch_no = 1.000000, Cost = 0.548620, Accuracy = 82.50 %\n",
      "ep:4, Batch_no = 2.000000, Cost = 0.490342, Accuracy = 85.00 %\n",
      "ep:4, Batch_no = 3.000000, Cost = 0.524173, Accuracy = 85.00 %\n",
      "ep:4, Batch_no = 4.000000, Cost = 0.420395, Accuracy = 87.50 %\n",
      "ep:4, Batch_no = 5.000000, Cost = 0.126973, Accuracy = 95.00 %\n",
      "ep:4, Batch_no = 6.000000, Cost = 0.218419, Accuracy = 95.00 %\n",
      "ep:4, Batch_no = 7.000000, Cost = 0.165971, Accuracy = 97.50 %\n",
      "ep:4, Batch_no = 8.000000, Cost = 0.514765, Accuracy = 80.00 %\n",
      "ep:4, Batch_no = 9.000000, Cost = 1.103422, Accuracy = 80.00 %\n",
      "ep:4, Batch_no = 10.000000, Cost = 0.657554, Accuracy = 87.50 %\n",
      "\n",
      "After epoch 4, Batch Cost = 0.657554, Batch Accuracy = 87.50 %\n",
      "\n",
      "ep:5, Batch_no = 1.000000, Cost = 0.344624, Accuracy = 92.50 %\n",
      "ep:5, Batch_no = 2.000000, Cost = 0.070839, Accuracy = 100.00 %\n",
      "ep:5, Batch_no = 3.000000, Cost = 0.322658, Accuracy = 92.50 %\n",
      "ep:5, Batch_no = 4.000000, Cost = 0.290583, Accuracy = 92.50 %\n",
      "ep:5, Batch_no = 5.000000, Cost = 0.644983, Accuracy = 85.00 %\n",
      "ep:5, Batch_no = 6.000000, Cost = 0.272147, Accuracy = 92.50 %\n",
      "ep:5, Batch_no = 7.000000, Cost = 0.219605, Accuracy = 95.00 %\n",
      "ep:5, Batch_no = 8.000000, Cost = 0.379251, Accuracy = 87.50 %\n",
      "ep:5, Batch_no = 9.000000, Cost = 0.589629, Accuracy = 87.50 %\n",
      "ep:5, Batch_no = 10.000000, Cost = 0.272091, Accuracy = 90.00 %\n",
      "\n",
      "After epoch 5, Batch Cost = 0.272091, Batch Accuracy = 90.00 %\n",
      "\n",
      "ep:6, Batch_no = 1.000000, Cost = 0.312377, Accuracy = 87.50 %\n",
      "ep:6, Batch_no = 2.000000, Cost = 0.267129, Accuracy = 87.50 %\n",
      "ep:6, Batch_no = 3.000000, Cost = 0.318398, Accuracy = 92.50 %\n",
      "ep:6, Batch_no = 4.000000, Cost = 0.234802, Accuracy = 95.00 %\n",
      "ep:6, Batch_no = 5.000000, Cost = 0.120127, Accuracy = 95.00 %\n",
      "ep:6, Batch_no = 6.000000, Cost = 0.166163, Accuracy = 95.00 %\n",
      "ep:6, Batch_no = 7.000000, Cost = 0.275378, Accuracy = 92.50 %\n",
      "ep:6, Batch_no = 8.000000, Cost = 0.179315, Accuracy = 95.00 %\n",
      "ep:6, Batch_no = 9.000000, Cost = 0.118079, Accuracy = 97.50 %\n",
      "ep:6, Batch_no = 10.000000, Cost = 0.104933, Accuracy = 95.00 %\n",
      "\n",
      "After epoch 6, Batch Cost = 0.104933, Batch Accuracy = 95.00 %\n",
      "\n",
      "ep:7, Batch_no = 1.000000, Cost = 0.091665, Accuracy = 100.00 %\n",
      "ep:7, Batch_no = 2.000000, Cost = 0.037846, Accuracy = 100.00 %\n",
      "ep:7, Batch_no = 3.000000, Cost = 0.138151, Accuracy = 97.50 %\n",
      "ep:7, Batch_no = 4.000000, Cost = 0.056905, Accuracy = 100.00 %\n",
      "ep:7, Batch_no = 5.000000, Cost = 0.035014, Accuracy = 100.00 %\n",
      "ep:7, Batch_no = 6.000000, Cost = 0.041509, Accuracy = 100.00 %\n",
      "ep:7, Batch_no = 7.000000, Cost = 0.083934, Accuracy = 97.50 %\n",
      "ep:7, Batch_no = 8.000000, Cost = 0.047461, Accuracy = 100.00 %\n",
      "ep:7, Batch_no = 9.000000, Cost = 0.112430, Accuracy = 97.50 %\n",
      "ep:7, Batch_no = 10.000000, Cost = 0.059518, Accuracy = 97.50 %\n",
      "\n",
      "After epoch 7, Batch Cost = 0.059518, Batch Accuracy = 97.50 %\n",
      "\n",
      "ep:8, Batch_no = 1.000000, Cost = 0.104291, Accuracy = 97.50 %\n",
      "ep:8, Batch_no = 2.000000, Cost = 0.133722, Accuracy = 95.00 %\n",
      "ep:8, Batch_no = 3.000000, Cost = 0.143524, Accuracy = 95.00 %\n",
      "ep:8, Batch_no = 4.000000, Cost = 0.037102, Accuracy = 100.00 %\n",
      "ep:8, Batch_no = 5.000000, Cost = 0.011287, Accuracy = 100.00 %\n",
      "ep:8, Batch_no = 6.000000, Cost = 0.028973, Accuracy = 100.00 %\n",
      "ep:8, Batch_no = 7.000000, Cost = 0.048611, Accuracy = 100.00 %\n",
      "ep:8, Batch_no = 8.000000, Cost = 0.031405, Accuracy = 100.00 %\n",
      "ep:8, Batch_no = 9.000000, Cost = 0.031174, Accuracy = 100.00 %\n",
      "ep:8, Batch_no = 10.000000, Cost = 0.058296, Accuracy = 97.50 %\n",
      "\n",
      "After epoch 8, Batch Cost = 0.058296, Batch Accuracy = 97.50 %\n",
      "\n",
      "ep:9, Batch_no = 1.000000, Cost = 0.037026, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 2.000000, Cost = 0.016929, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 3.000000, Cost = 0.073242, Accuracy = 97.50 %\n",
      "ep:9, Batch_no = 4.000000, Cost = 0.023675, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 5.000000, Cost = 0.018042, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 6.000000, Cost = 0.019260, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 7.000000, Cost = 0.026842, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 8.000000, Cost = 0.020075, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 9.000000, Cost = 0.021829, Accuracy = 100.00 %\n",
      "ep:9, Batch_no = 10.000000, Cost = 0.015698, Accuracy = 100.00 %\n",
      "\n",
      "After epoch 9, Batch Cost = 0.015698, Batch Accuracy = 100.00 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W1_t,b1_t,theta_t,bias_t,cost_t,accuracy_t = main_init(t,W1,b1,theta,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
