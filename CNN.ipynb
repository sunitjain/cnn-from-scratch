{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and reshape accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "#(42000, 784)\n",
    "test_data.shape\n",
    "#(28000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data.label\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.get_dummies(label, columns=['label'], drop_first=False)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(train_data,target,test_size = 0.25, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 784)\n",
      "(10500, 784)\n",
      "31500\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_cv.shape\n",
    "print len(X_train)\n",
    "#(31500, 784)\n",
    "#(10500, 784)\n",
    "#31500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the DF to input image\n",
    "\n",
    "x_arr = np.array(X_train)\n",
    "x_cv_arr = np.array(X_cv)\n",
    "X = x_arr.reshape(len(X_train),28,28,1)\n",
    "X_cv = x_cv_arr.reshape(len(X_cv),28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31500, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PADDING function\n",
    "def zero_pad(data, pad):\n",
    "    data_pad = np.pad(data,((0,0),(pad,pad),(pad,pad),(0,0)), 'constant')\n",
    "    \n",
    "    return data_pad\n",
    "#zero_pad(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxpooling function\n",
    "def max_pool(X,f,stride):\n",
    "    (m, w, w, c) = X.shape\n",
    "    output_size = int((w-f)/stride+1)\n",
    "    pool = np.zeros((m,output_size,output_size, c))\n",
    "    for e in range(0,m):\n",
    "        for k in range(0,c):\n",
    "            #i=0\n",
    "            #i = int(i)\n",
    "            for i in range(output_size):\n",
    "                #j=0\n",
    "                #j = int(j)\n",
    "                for j in range(output_size):\n",
    "                    pool[e,i,j,k] = np.max(X[e,i*stride:i*stride+f,j*stride:j*stride+f,k])\n",
    "                    #j+=stride\n",
    "                #i+=stride\n",
    "    return pool\n",
    "    #return pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_pool(X, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(in_mat, Y, W1, W2, b1, b2, theta3, bias3):\n",
    "    \n",
    "    # forward propagation\n",
    "    \n",
    "    # input\n",
    "    m, w, w, c = in_mat.shape\n",
    "    # no of filters in lay1 and lay2\n",
    "    l1 = len(W1)\n",
    "    l2 = len(W2)\n",
    "    \n",
    "    # shape of the filter\n",
    "    (f, f, _) = W1[0].shape\n",
    "    pad = 1\n",
    "    stride = 1\n",
    "    \n",
    "    # conv output\n",
    "    nw1 = (w + (2*pad) - f)/stride + 1\n",
    "    nw2 = (nw1 - f)/stride + 1 \n",
    "    \n",
    "    # init output image mats\n",
    "    conv1 = np.zeros((m, nw1, nw1, l1))\n",
    "    conv2 = np.zeros((m, nw2, nw2, l1))\n",
    "    \n",
    "    # pad\n",
    "    pad_mat = zero_pad(in_mat,pad)\n",
    "    \n",
    "    # first conv layer\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l1):\n",
    "            for k in range(0,nw1):\n",
    "                for l in range(0,nw1):\n",
    "                    conv1[i,k,l,j] = np.sum(pad_mat[i,k:k+f,l:l+f]*W1[j]) + b1[j]\n",
    "        conv1[i,:,:,:][conv1[i,:,:,:] <= 0] = 0\n",
    "    \n",
    "    # second conv layer\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l2):\n",
    "            for k in range(0,nw2):\n",
    "                for l in range(0,nw2):\n",
    "                    conv2[i,k,l,j] = np.sum(conv1[i,k:k+f,l:l+f,:]*W2[j]) + b2[j]\n",
    "        conv2[i,:,:,:][conv2[i,:,:,:] <= 0] = 0\n",
    "\n",
    "    # pooling filter = 2, stride = 2\n",
    "    pooled_layer = max_pool(conv2, 2, 2)\n",
    "    \n",
    "    ## Fully connected layer of neurons\n",
    "    fully_connected = pooled_layer.reshape(m,int(int(nw2/2)*int(nw2/2)*l2))\n",
    "    \n",
    "    ## Output layer of mx10 activation units\n",
    "    #theta3 = initialize_theta3(params = int((nw2/2)*(nw2/2)*l2))\n",
    "    out = np.dot(fully_connected,theta3) + bias3\n",
    "        \n",
    "    ## Using softmax to get the cost    \n",
    "    p, cost, probs, prob_label = softmax_cost(out, Y)   ## change it to y_train or batch\n",
    "    \n",
    "    acc = []\n",
    "    for i in range(0,len(Y)):\n",
    "        if prob_label[i]==np.argmax(np.array(Y)[i,:]):\n",
    "            acc.append(1)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "\n",
    "    # backpropagation layer\n",
    "    \n",
    "    dout = probs - Y\n",
    "    \n",
    "    dtheta3 = np.dot(dout.T, fully_connected)\n",
    "    dbias3 = np.mean(dout, axis = 0).reshape(1,10)\n",
    "    \n",
    "    dfully_connected = np.dot(theta3, dout.T)\n",
    "    \n",
    "    dpool = dfully_connected.T.reshape((m, int(nw2/2), int(nw2/2), l2))\n",
    "    \n",
    "    dconv2 = np.zeros((m, nw2, nw2, l2))\n",
    "    \n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    \n",
    "\n",
    "    dconv1 = np.zeros((m, nw1, nw1, l1))\n",
    "    \n",
    "    dW2_stack = {}\n",
    "    db2_stack = {}\n",
    "    dW2_stack = np.zeros((m,l2,f,f,l1))\n",
    "    db2_stack = np.zeros((m,l2,1))\n",
    "\n",
    "    dW1_stack = {}\n",
    "    db1_stack = {}\n",
    "    dW1_stack = np.zeros((m,l1,f,f,1))\n",
    "    db1_stack = np.zeros((m,l1,1))\n",
    "\n",
    "    dW2 = {}\n",
    "    dW2 = np.zeros((l2,f,f,l1))\n",
    "    db2 = np.zeros((l2,1))\n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    \n",
    "    # calculate the mean gradient for each batch of examples\n",
    "    # looping through the one batch of 32 examples\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l2):\n",
    "            for k in range(0,nw2):\n",
    "                for l in range(0,nw2):\n",
    "                    dW2_stack[i,:,:,j] += dconv2[i,k,l,j]*conv1[i,k:k+f,l:l+f,:]\n",
    "                    dconv1[i,k:k+f,l:l+f,:] += dconv2[i,k,l,j]*W2[j]\n",
    "            db2_stack[i,j] = np.sum(dconv2[i,:,:,j])\n",
    "        dconv1[conv1<=0]=0\n",
    "        \n",
    "        # calculating the mean gradient\n",
    "        dW2 = np.mean(dW2_stack, axis = 0)  \n",
    "        db2 = np.mean(db2_stack, axis = 0)\n",
    "    \n",
    "    # again looping through the one batch of 32 examples\n",
    "    \n",
    "    for i in range(0,m):                          \n",
    "        for j in range(0,l1):\n",
    "            for k in range(0,nw1):\n",
    "                for l in range(0,nw1):\n",
    "                    dW1_stack[i,:,:,j] += dconv1[i,k,l,j]*pad_mat[i,k:k+f,l:l+f,:]\n",
    "\n",
    "            db1_stack[i,j] = np.sum(dconv1[i,:,:,j])\n",
    "            \n",
    "        dW1 = np.mean(dW1_stack, axis = 0)\n",
    "        db1 = np.mean(db1_stack, axis = 0)\n",
    "\n",
    "        \n",
    "        \n",
    "    return dW1, dW2, db1, db2, dtheta3, dbias3, cost, probs, prob_label, acc        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(batch,learning_rate,W1,W2,b1,b2,theta3,bias3):\n",
    "    \n",
    "    ## Slicing train data and labels from batch\n",
    "    X = batch[:,0:-10]\n",
    "    X = X.reshape(len(batch), w, w, l)\n",
    "    Y = batch[:,784:794]\n",
    "    \n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    ## Initializing gradient matrices \n",
    "    dW2 = {}\n",
    "    dW2 = np.zeros((l2,f,f,l1))\n",
    "    db2 = np.zeros((l2,1))\n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    dtheta3 = np.zeros(theta3.shape)\n",
    "    dbias3 = np.zeros(bias3.shape)\n",
    "    \n",
    "    grads = conv_layer(X,Y,W1,W2,b1,b2,theta3,bias3)\n",
    "    [dW1, dW2, db1, db2, dtheta3, dbias3, cost_, probs_, prob_label, acc_] = grads\n",
    "    \n",
    "    W1 = W1-learning_rate*(dW1)\n",
    "    b1 = b1-learning_rate*(db1)\n",
    "    W2 = W2-learning_rate*(dW2)\n",
    "    b2 = b2-learning_rate*(db2)\n",
    "    theta3 = theta3-learning_rate*(dtheta3.T)\n",
    "    bias3 = bias3-learning_rate*(dbias3)\n",
    "    \n",
    "    batch_cost = np.mean(cost_)\n",
    "    #print dW1    ## checking if the gradients are calculated or not (yes)\n",
    "    batch_accuracy = sum(acc_)/len(acc_)\n",
    "    \n",
    "    return W1, W2, b1, b2, theta3, bias3, batch_cost, acc_, batch_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Convolution layer\\n\\ndef conv_layer(in_mat, recep_size, stride, zero_pad, no_filters, weight, bias):\\n    \\n    F = recep_size\\n    S = stride\\n    P = zero_pad\\n    K = no_filters\\n    \\n    in_h = in_mat.shape[0]\\n    in_w = in_mat.shape[1]\\n    in_d = in_mat.shape[2]\\n    \\n    W = weight\\n    B = bias\\n    \\n    out_h = (in_h + 2*P - F)/S + 1\\n    out_w = (in_w + 2*P - F)/S + 1\\n    out_d = K\\n    \\n    out_mat = np.empty((out_h,out_w,out_d))\\n    \\n    for d in range(out_d):\\n        for i in range(out_h):\\n            for j in range(out_w):\\n                out_img[i,j,d] = np.sum(in_mat[i*S:i*S + F, j*S:j*S + F,:] * W[d,:,:,:]) + 1\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Convolution layer\n",
    "\n",
    "def conv_layer(in_mat, recep_size, stride, zero_pad, no_filters, weight, bias):\n",
    "    \n",
    "    F = recep_size\n",
    "    S = stride\n",
    "    P = zero_pad\n",
    "    K = no_filters\n",
    "    \n",
    "    in_h = in_mat.shape[0]\n",
    "    in_w = in_mat.shape[1]\n",
    "    in_d = in_mat.shape[2]\n",
    "    \n",
    "    W = weight\n",
    "    B = bias\n",
    "    \n",
    "    out_h = (in_h + 2*P - F)/S + 1\n",
    "    out_w = (in_w + 2*P - F)/S + 1\n",
    "    out_d = K\n",
    "    \n",
    "    out_mat = np.empty((out_h,out_w,out_d))\n",
    "    \n",
    "    for d in range(out_d):\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                out_img[i,j,d] = np.sum(in_mat[i*S:i*S + F, j*S:j*S + F,:] * W[d,:,:,:]) + 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "\n",
    "#def softmax(X):\n",
    "    #softm = np.exp(X)/np.sum(np.exp(X),axis=1,keepdims=True)\n",
    "    #return softm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(out,y):\n",
    "    #softm = np.exp(X)/np.sum(np.exp(X),axis=1,keepdims=True)\n",
    "    #return softm\n",
    "    eout = np.exp(out, dtype=np.float)  \n",
    "    probs = eout/np.sum(eout, axis = 1)[:,None]\n",
    "    \n",
    "    p = np.sum(np.multiply(y,probs), axis = 1)\n",
    "    prob_label = np.argmax(np.array(probs), axis = 1)    ## taking out the arguments of max values\n",
    "    cost = -np.log(p)    ## (Only data loss. No regularised loss)\n",
    "    \n",
    "    return p,cost,probs,prob_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing weights and bias\n",
    "\n",
    "W1 = 0.1*np.random.rand(3,3,3,1)\n",
    "W2 = 0.1*np.random.rand(3,3,3,3)\n",
    "\n",
    "b1 = 0.1*np.random.rand(3,1)\n",
    "b2 = 0.1*np.random.rand(3,1)\n",
    "\n",
    "## Initializing weights and bias for fully connected layer\n",
    "\n",
    "theta3 = 0.1*np.random.rand(507,10)\n",
    "bias3 = 0.1*np.random.rand(1,10)\n",
    "\n",
    "## Normalizing input data\n",
    "x_arr -= int(np.mean(x_arr))\n",
    "x_arr = x_arr.astype(float)\n",
    "x_arr /= int(np.std(x_arr))\n",
    "\n",
    "train_data = np.hstack((x_arr,np.array(y_train)))     ## horizontally stacking the features and labels \n",
    "t = train_data[0:320]      ## training the model on only 300 examples images due to heavy computation issue\n",
    "\n",
    "## Normalizing cross-validation data\n",
    "x_cv_arr -= int(np.mean(x_cv_arr))\n",
    "x_cv_arr = x_cv_arr.astype(float)\n",
    "x_cv_arr /= int(np.std(x_cv_arr))\n",
    "\n",
    "cv_data = np.hstack((x_cv_arr,np.array(y_cv)))\n",
    "test_data = x_cv_arr[0:100]    ## cross-validating the model on only 100 examples images due to heavy computation issue   \n",
    "Y_cv = np.array(y_cv)[0:100]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "## Assigning hyperparameter values\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "num_images = len(t)   ##Number of the input training examples\n",
    "w = 28\n",
    "l = 1\n",
    "l1 = len(W1)    ## no. opf filters in W1 and W2 \n",
    "l2 = len(W2)    \n",
    "f = len(W1[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_init(train_data,W1,W2,b1,b2,theta3,bias3):\n",
    "    cost = []\n",
    "    accuracy = []\n",
    "    for epoch in range(0, num_epochs):\n",
    "        batches = [train_data[k:k + batch_size] for k in xrange(0, len(train_data), batch_size)]\n",
    "        x=0\n",
    "        i = 1\n",
    "        for batch in batches:\n",
    "            \n",
    "            output = optimizer(batch,learning_rate,W1,W2,b1,b2,theta3,bias3)\n",
    "            [W1, W2, b1, b2, theta3, bias3, batch_cost,acc_,batch_acc] = output\n",
    "            \n",
    "            #epoch_acc = round(np.sum(accuracy[epoch*num_images/batch_size:])/(x+1),2)\n",
    "            \n",
    "            cost.append(batch_cost)\n",
    "            accuracy.append(batch_acc)\n",
    "\n",
    "            print 'ep:%d, batch_num = %f, batch_cost = %f, batch_acc = %f' %(epoch,i,batch_cost,batch_acc) \n",
    "            i+=1\n",
    "\n",
    "    \n",
    "    return W1,W2,b1,b2,theta3,bias3,cost,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:0, batch_num = 1.000000, batch_cost = 2.374285, batch_acc = 0.000000\n",
      "ep:0, batch_num = 2.000000, batch_cost = 5.456819, batch_acc = 0.000000\n",
      "ep:0, batch_num = 3.000000, batch_cost = 9.566067, batch_acc = 0.000000\n",
      "ep:0, batch_num = 4.000000, batch_cost = 4.886193, batch_acc = 0.000000\n",
      "ep:0, batch_num = 5.000000, batch_cost = 5.285850, batch_acc = 0.000000\n",
      "ep:0, batch_num = 6.000000, batch_cost = 6.175682, batch_acc = 0.000000\n",
      "ep:0, batch_num = 7.000000, batch_cost = 3.977008, batch_acc = 0.000000\n",
      "ep:0, batch_num = 8.000000, batch_cost = 3.071404, batch_acc = 0.000000\n",
      "ep:0, batch_num = 9.000000, batch_cost = 2.742477, batch_acc = 0.000000\n",
      "ep:0, batch_num = 10.000000, batch_cost = 3.057887, batch_acc = 0.000000\n",
      "ep:1, batch_num = 1.000000, batch_cost = 1.838196, batch_acc = 0.000000\n",
      "ep:1, batch_num = 2.000000, batch_cost = 2.100013, batch_acc = 0.000000\n",
      "ep:1, batch_num = 3.000000, batch_cost = 0.693553, batch_acc = 0.000000\n",
      "ep:1, batch_num = 4.000000, batch_cost = 1.208435, batch_acc = 0.000000\n",
      "ep:1, batch_num = 5.000000, batch_cost = 2.722554, batch_acc = 0.000000\n",
      "ep:1, batch_num = 6.000000, batch_cost = 1.164829, batch_acc = 0.000000\n",
      "ep:1, batch_num = 7.000000, batch_cost = 1.392531, batch_acc = 0.000000\n",
      "ep:1, batch_num = 8.000000, batch_cost = 1.381682, batch_acc = 0.000000\n",
      "ep:1, batch_num = 9.000000, batch_cost = 1.159142, batch_acc = 0.000000\n",
      "ep:1, batch_num = 10.000000, batch_cost = 1.031860, batch_acc = 0.000000\n",
      "ep:2, batch_num = 1.000000, batch_cost = 0.712321, batch_acc = 0.000000\n",
      "ep:2, batch_num = 2.000000, batch_cost = 0.760352, batch_acc = 0.000000\n",
      "ep:2, batch_num = 3.000000, batch_cost = 0.365082, batch_acc = 0.000000\n",
      "ep:2, batch_num = 4.000000, batch_cost = 0.404568, batch_acc = 0.000000\n",
      "ep:2, batch_num = 5.000000, batch_cost = 0.519038, batch_acc = 0.000000\n",
      "ep:2, batch_num = 6.000000, batch_cost = 0.583322, batch_acc = 0.000000\n",
      "ep:2, batch_num = 7.000000, batch_cost = 1.005609, batch_acc = 0.000000\n",
      "ep:2, batch_num = 8.000000, batch_cost = 1.024094, batch_acc = 0.000000\n",
      "ep:2, batch_num = 9.000000, batch_cost = 0.888284, batch_acc = 0.000000\n",
      "ep:2, batch_num = 10.000000, batch_cost = 0.797643, batch_acc = 0.000000\n",
      "ep:3, batch_num = 1.000000, batch_cost = 0.565499, batch_acc = 0.000000\n",
      "ep:3, batch_num = 2.000000, batch_cost = 0.558503, batch_acc = 0.000000\n",
      "ep:3, batch_num = 3.000000, batch_cost = 0.337317, batch_acc = 0.000000\n",
      "ep:3, batch_num = 4.000000, batch_cost = 0.412792, batch_acc = 0.000000\n",
      "ep:3, batch_num = 5.000000, batch_cost = 0.597321, batch_acc = 0.000000\n",
      "ep:3, batch_num = 6.000000, batch_cost = 0.517857, batch_acc = 0.000000\n",
      "ep:3, batch_num = 7.000000, batch_cost = 0.724185, batch_acc = 0.000000\n",
      "ep:3, batch_num = 8.000000, batch_cost = 0.860667, batch_acc = 0.000000\n",
      "ep:3, batch_num = 9.000000, batch_cost = 0.731417, batch_acc = 0.000000\n",
      "ep:3, batch_num = 10.000000, batch_cost = 0.615058, batch_acc = 0.000000\n",
      "ep:4, batch_num = 1.000000, batch_cost = 0.503181, batch_acc = 0.000000\n",
      "ep:4, batch_num = 2.000000, batch_cost = 0.538175, batch_acc = 0.000000\n",
      "ep:4, batch_num = 3.000000, batch_cost = 0.485183, batch_acc = 0.000000\n",
      "ep:4, batch_num = 4.000000, batch_cost = 0.696014, batch_acc = 0.000000\n",
      "ep:4, batch_num = 5.000000, batch_cost = 1.090246, batch_acc = 0.000000\n",
      "ep:4, batch_num = 6.000000, batch_cost = 0.341448, batch_acc = 0.000000\n",
      "ep:4, batch_num = 7.000000, batch_cost = 0.747334, batch_acc = 0.000000\n",
      "ep:4, batch_num = 8.000000, batch_cost = 0.753272, batch_acc = 0.000000\n",
      "ep:4, batch_num = 9.000000, batch_cost = 0.669713, batch_acc = 0.000000\n",
      "ep:4, batch_num = 10.000000, batch_cost = 0.547338, batch_acc = 0.000000\n",
      "ep:5, batch_num = 1.000000, batch_cost = 0.444789, batch_acc = 0.000000\n",
      "ep:5, batch_num = 2.000000, batch_cost = 0.416085, batch_acc = 0.000000\n",
      "ep:5, batch_num = 3.000000, batch_cost = 0.279118, batch_acc = 0.000000\n",
      "ep:5, batch_num = 4.000000, batch_cost = 0.262193, batch_acc = 0.000000\n",
      "ep:5, batch_num = 5.000000, batch_cost = 0.306794, batch_acc = 0.000000\n",
      "ep:5, batch_num = 6.000000, batch_cost = 0.396874, batch_acc = 0.000000\n",
      "ep:5, batch_num = 7.000000, batch_cost = 0.980609, batch_acc = 0.000000\n",
      "ep:5, batch_num = 8.000000, batch_cost = 0.888222, batch_acc = 0.000000\n",
      "ep:5, batch_num = 9.000000, batch_cost = 0.506668, batch_acc = 0.000000\n",
      "ep:5, batch_num = 10.000000, batch_cost = 0.476480, batch_acc = 0.000000\n",
      "ep:6, batch_num = 1.000000, batch_cost = 0.495832, batch_acc = 0.000000\n",
      "ep:6, batch_num = 2.000000, batch_cost = 0.499198, batch_acc = 0.000000\n",
      "ep:6, batch_num = 3.000000, batch_cost = 0.546847, batch_acc = 0.000000\n",
      "ep:6, batch_num = 4.000000, batch_cost = 0.513230, batch_acc = 0.000000\n",
      "ep:6, batch_num = 5.000000, batch_cost = 0.550100, batch_acc = 0.000000\n",
      "ep:6, batch_num = 6.000000, batch_cost = 0.305503, batch_acc = 0.000000\n",
      "ep:6, batch_num = 7.000000, batch_cost = 0.631631, batch_acc = 0.000000\n",
      "ep:6, batch_num = 8.000000, batch_cost = 0.569419, batch_acc = 0.000000\n",
      "ep:6, batch_num = 9.000000, batch_cost = 0.568347, batch_acc = 0.000000\n",
      "ep:6, batch_num = 10.000000, batch_cost = 0.470010, batch_acc = 0.000000\n",
      "ep:7, batch_num = 1.000000, batch_cost = 0.400001, batch_acc = 0.000000\n",
      "ep:7, batch_num = 2.000000, batch_cost = 0.322130, batch_acc = 0.000000\n",
      "ep:7, batch_num = 3.000000, batch_cost = 0.236137, batch_acc = 0.000000\n",
      "ep:7, batch_num = 4.000000, batch_cost = 0.227757, batch_acc = 0.000000\n",
      "ep:7, batch_num = 5.000000, batch_cost = 0.254143, batch_acc = 0.000000\n",
      "ep:7, batch_num = 6.000000, batch_cost = 0.327099, batch_acc = 0.000000\n",
      "ep:7, batch_num = 7.000000, batch_cost = 0.842451, batch_acc = 0.000000\n",
      "ep:7, batch_num = 8.000000, batch_cost = 0.697610, batch_acc = 0.000000\n",
      "ep:7, batch_num = 9.000000, batch_cost = 0.459126, batch_acc = 0.000000\n",
      "ep:7, batch_num = 10.000000, batch_cost = 0.435985, batch_acc = 0.000000\n",
      "ep:8, batch_num = 1.000000, batch_cost = 0.449051, batch_acc = 0.000000\n",
      "ep:8, batch_num = 2.000000, batch_cost = 0.352223, batch_acc = 0.000000\n",
      "ep:8, batch_num = 3.000000, batch_cost = 0.259424, batch_acc = 0.000000\n",
      "ep:8, batch_num = 4.000000, batch_cost = 0.211361, batch_acc = 0.000000\n",
      "ep:8, batch_num = 5.000000, batch_cost = 0.270583, batch_acc = 0.000000\n",
      "ep:8, batch_num = 6.000000, batch_cost = 0.297677, batch_acc = 0.000000\n",
      "ep:8, batch_num = 7.000000, batch_cost = 0.742651, batch_acc = 0.000000\n",
      "ep:8, batch_num = 8.000000, batch_cost = 0.576149, batch_acc = 0.000000\n",
      "ep:8, batch_num = 9.000000, batch_cost = 0.453845, batch_acc = 0.000000\n",
      "ep:8, batch_num = 10.000000, batch_cost = 0.412115, batch_acc = 0.000000\n",
      "ep:9, batch_num = 1.000000, batch_cost = 0.408157, batch_acc = 0.000000\n",
      "ep:9, batch_num = 2.000000, batch_cost = 0.294927, batch_acc = 0.000000\n",
      "ep:9, batch_num = 3.000000, batch_cost = 0.217100, batch_acc = 0.000000\n",
      "ep:9, batch_num = 4.000000, batch_cost = 0.200349, batch_acc = 0.000000\n",
      "ep:9, batch_num = 5.000000, batch_cost = 0.252994, batch_acc = 0.000000\n",
      "ep:9, batch_num = 6.000000, batch_cost = 0.259663, batch_acc = 0.000000\n",
      "ep:9, batch_num = 7.000000, batch_cost = 0.650916, batch_acc = 0.000000\n",
      "ep:9, batch_num = 8.000000, batch_cost = 0.493040, batch_acc = 0.000000\n",
      "ep:9, batch_num = 9.000000, batch_cost = 0.427320, batch_acc = 0.000000\n",
      "ep:9, batch_num = 10.000000, batch_cost = 0.396097, batch_acc = 0.000000\n"
     ]
    }
   ],
   "source": [
    "W1_t,W2_t,b1_t,b2_t,theta3_t,bias3_t,cost_t,accuracy_t = main_init(t,W1,W2,b1,b2,theta3,bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
