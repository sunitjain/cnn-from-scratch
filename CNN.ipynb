{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and reshape accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "#(42000, 784)\n",
    "test_data.shape\n",
    "#(28000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data.label\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.get_dummies(label, columns=['label'], drop_first=False)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(train_data,target,test_size = 0.25, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 784)\n",
      "(10500, 784)\n",
      "31500\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_cv.shape\n",
    "print len(X_train)\n",
    "#(31500, 784)\n",
    "#(10500, 784)\n",
    "#31500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the DF to input image\n",
    "\n",
    "x_arr = np.array(X_train)\n",
    "x_cv_arr = np.array(X_cv)\n",
    "X = x_arr.reshape(len(X_train),28,28,1)\n",
    "X_cv = x_cv_arr.reshape(len(X_cv),28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31500, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PADDING function\n",
    "def zero_pad(data, pad):\n",
    "    data_pad = np.pad(data,((0,0),(pad,pad),(pad,pad),(0,0)), 'constant')\n",
    "    \n",
    "    return data_pad\n",
    "#zero_pad(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxpooling function\n",
    "def max_pool(X,f,stride):\n",
    "    (m, w, w, c) = X.shape\n",
    "    output_size = int((w-f)/stride+1)\n",
    "    pool = np.zeros((m,output_size,output_size, c))\n",
    "    for e in range(0,m):\n",
    "        for k in range(0,c):\n",
    "            #i=0\n",
    "            #i = int(i)\n",
    "            for i in range(output_size):\n",
    "                #j=0\n",
    "                #j = int(j)\n",
    "                for j in range(output_size):\n",
    "                    pool[e,i,j,k] = np.max(X[e,i*stride:i*stride+f,j*stride:j*stride+f,k])\n",
    "                    #j+=stride\n",
    "                #i+=stride\n",
    "    return pool\n",
    "    #return pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_pool(X, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(in_mat, Y, W1, W2, b1, b2, theta3, bias3):\n",
    "    \n",
    "    # forward propagation\n",
    "    \n",
    "    # input\n",
    "    m, w, w, c = in_mat.shape\n",
    "    # no of filters in lay1 and lay2\n",
    "    l1 = len(W1)\n",
    "    l2 = len(W2)\n",
    "    \n",
    "    # shape of the filter\n",
    "    (f, f, _) = W1[0].shape\n",
    "    pad = 1\n",
    "    stride = 1\n",
    "    \n",
    "    # conv output\n",
    "    nw1 = (w + (2*pad) - f)/stride + 1\n",
    "    nw2 = (nw1 - f)/stride + 1 \n",
    "    \n",
    "    # init output image mats\n",
    "    conv1 = np.zeros((m, nw1, nw1, l1))\n",
    "    conv2 = np.zeros((m, nw2, nw2, l1))\n",
    "    \n",
    "    # pad\n",
    "    pad_mat = zero_pad(in_mat,pad)\n",
    "    \n",
    "    # first conv layer\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l1):\n",
    "            for k in range(0,nw1):\n",
    "                for l in range(0,nw1):\n",
    "                    conv1[i,k,l,j] = np.sum(pad_mat[i,k:k+f,l:l+f]*W1[j]) + b1[j]\n",
    "        conv1[i,:,:,:][conv1[i,:,:,:] <= 0] = 0\n",
    "    \n",
    "    # second conv layer\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l2):\n",
    "            for k in range(0,nw2):\n",
    "                for l in range(0,nw2):\n",
    "                    conv2[i,k,l,j] = np.sum(conv1[i,k:k+f,l:l+f,:]*W2[j]) + b2[j]\n",
    "        conv2[i,:,:,:][conv2[i,:,:,:] <= 0] = 0\n",
    "\n",
    "    # pooling filter = 2, stride = 2\n",
    "    pooled_layer = max_pool(conv2, 2, 2)\n",
    "    \n",
    "    ## Fully connected layer of neurons\n",
    "    fully_connected = pooled_layer.reshape(m,int(int(nw2/2)*int(nw2/2)*l2))\n",
    "    \n",
    "    ## Output layer of mx10 activation units\n",
    "    #theta3 = initialize_theta3(params = int((nw2/2)*(nw2/2)*l2))\n",
    "    out = np.dot(fully_connected,theta3) + bias3\n",
    "        \n",
    "    ## Using softmax to get the cost    \n",
    "    p, cost, probs, prob_label = softmax_cost(out, Y)   ## change it to y_train or batch\n",
    "    \n",
    "    acc = []\n",
    "    for i in range(0,len(Y)):\n",
    "        if prob_label[i]==np.argmax(np.array(Y)[i,:]):\n",
    "            acc.append(1)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "\n",
    "    # backpropagation layer\n",
    "    \n",
    "    dout = probs - Y\n",
    "    \n",
    "    dtheta3 = np.dot(dout.T, fully_connected)\n",
    "    dbias3 = np.mean(dout, axis = 0).reshape(1,10)\n",
    "    \n",
    "    dfully_connected = np.dot(theta3, dout.T)\n",
    "    \n",
    "    dpool = dfully_connected.T.reshape((m, int(nw2/2), int(nw2/2), l2))\n",
    "    \n",
    "    dconv2 = np.zeros((m, nw2, nw2, l2))\n",
    "    \n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    # for pooled numbers/indices, implement bp\n",
    "    \n",
    "\n",
    "    dconv1 = np.zeros((m, nw1, nw1, l1))\n",
    "    \n",
    "    dW2_stack = {}\n",
    "    db2_stack = {}\n",
    "    dW2_stack = np.zeros((m,l2,f,f,l1))\n",
    "    db2_stack = np.zeros((m,l2,1))\n",
    "\n",
    "    dW1_stack = {}\n",
    "    db1_stack = {}\n",
    "    dW1_stack = np.zeros((m,l1,f,f,1))\n",
    "    db1_stack = np.zeros((m,l1,1))\n",
    "\n",
    "    dW2 = {}\n",
    "    dW2 = np.zeros((l2,f,f,l1))\n",
    "    db2 = np.zeros((l2,1))\n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    \n",
    "    # calculate the mean gradient for each batch of examples\n",
    "    # looping through the one batch of 32 examples\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        for j in range(0,l2):\n",
    "            for k in range(0,nw2):\n",
    "                for l in range(0,nw2):\n",
    "                    dW2_stack[i,:,:,j] += dconv2[i,k,l,j]*conv1[i,k:k+f,l:l+f,:]\n",
    "                    dconv1[i,k:k+f,l:l+f,:] += dconv2[i,k,l,j]*W2[j]\n",
    "            db2_stack[i,j] = np.sum(dconv2[i,:,:,j])\n",
    "        dconv1[conv1<=0]=0\n",
    "        \n",
    "        # calculating the mean gradient\n",
    "        dW2 = np.mean(dW2_stack, axis = 0)  \n",
    "        db2 = np.mean(db2_stack, axis = 0)\n",
    "    \n",
    "    # again looping through the one batch of 32 examples\n",
    "    \n",
    "    for i in range(0,m):                          \n",
    "        for j in range(0,l1):\n",
    "            for k in range(0,nw1):\n",
    "                for l in range(0,nw1):\n",
    "                    dW1_stack[i,:,:,j] += dconv1[i,k,l,j]*pad_mat[i,k:k+f,l:l+f,:]\n",
    "\n",
    "            db1_stack[i,j] = np.sum(dconv1[i,:,:,j])\n",
    "            \n",
    "        dW1 = np.mean(dW1_stack, axis = 0)\n",
    "        db1 = np.mean(db1_stack, axis = 0)\n",
    "\n",
    "        \n",
    "        \n",
    "    return dW1, dW2, db1, db2, dtheta3, dbias3, cost, probs, prob_label, acc        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(batch,learning_rate,W1,W2,b1,b2,theta3,bias3):\n",
    "    \n",
    "    ## Slicing train data and labels from batch\n",
    "    X = batch[:,0:-10]\n",
    "    X = X.reshape(len(batch), w, w, l)\n",
    "    Y = batch[:,784:794]\n",
    "    \n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    ## Initializing gradient matrices \n",
    "    dW2 = {}\n",
    "    dW2 = np.zeros((l2,f,f,l1))\n",
    "    db2 = np.zeros((l2,1))\n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    dtheta3 = np.zeros(theta3.shape)\n",
    "    dbias3 = np.zeros(bias3.shape)\n",
    "    \n",
    "    grads = conv_layer(X,Y,W1,W2,b1,b2,theta3,bias3)\n",
    "    [dW1, dW2, db1, db2, dtheta3, dbias3, cost_, probs_, prob_label, acc_] = grads\n",
    "    \n",
    "    W1 = W1-learning_rate*(dW1)\n",
    "    b1 = b1-learning_rate*(db1)\n",
    "    W2 = W2-learning_rate*(dW2)\n",
    "    b2 = b2-learning_rate*(db2)\n",
    "    theta3 = theta3-learning_rate*(dtheta3.T)\n",
    "    bias3 = bias3-learning_rate*(dbias3)\n",
    "    \n",
    "    batch_cost = np.mean(cost_)\n",
    "    #print dW1    ## checking if the gradients are calculated or not (yes)\n",
    "    batch_accuracy = sum(acc_)/len(acc_)\n",
    "    \n",
    "    return W1, W2, b1, b2, theta3, bias3, batch_cost, acc_, batch_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Convolution layer\\n\\ndef conv_layer(in_mat, recep_size, stride, zero_pad, no_filters, weight, bias):\\n    \\n    F = recep_size\\n    S = stride\\n    P = zero_pad\\n    K = no_filters\\n    \\n    in_h = in_mat.shape[0]\\n    in_w = in_mat.shape[1]\\n    in_d = in_mat.shape[2]\\n    \\n    W = weight\\n    B = bias\\n    \\n    out_h = (in_h + 2*P - F)/S + 1\\n    out_w = (in_w + 2*P - F)/S + 1\\n    out_d = K\\n    \\n    out_mat = np.empty((out_h,out_w,out_d))\\n    \\n    for d in range(out_d):\\n        for i in range(out_h):\\n            for j in range(out_w):\\n                out_img[i,j,d] = np.sum(in_mat[i*S:i*S + F, j*S:j*S + F,:] * W[d,:,:,:]) + 1\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Convolution layer\n",
    "\n",
    "def conv_layer(in_mat, recep_size, stride, zero_pad, no_filters, weight, bias):\n",
    "    \n",
    "    F = recep_size\n",
    "    S = stride\n",
    "    P = zero_pad\n",
    "    K = no_filters\n",
    "    \n",
    "    in_h = in_mat.shape[0]\n",
    "    in_w = in_mat.shape[1]\n",
    "    in_d = in_mat.shape[2]\n",
    "    \n",
    "    W = weight\n",
    "    B = bias\n",
    "    \n",
    "    out_h = (in_h + 2*P - F)/S + 1\n",
    "    out_w = (in_w + 2*P - F)/S + 1\n",
    "    out_d = K\n",
    "    \n",
    "    out_mat = np.empty((out_h,out_w,out_d))\n",
    "    \n",
    "    for d in range(out_d):\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                out_img[i,j,d] = np.sum(in_mat[i*S:i*S + F, j*S:j*S + F,:] * W[d,:,:,:]) + 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "\n",
    "#def softmax(X):\n",
    "    #softm = np.exp(X)/np.sum(np.exp(X),axis=1,keepdims=True)\n",
    "    #return softm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(out,y):\n",
    "    #softm = np.exp(X)/np.sum(np.exp(X),axis=1,keepdims=True)\n",
    "    #return softm\n",
    "    eout = np.exp(out, dtype=np.float)  \n",
    "    probs = eout/np.sum(eout, axis = 1)[:,None]\n",
    "    \n",
    "    p = np.sum(np.multiply(y,probs), axis = 1)\n",
    "    prob_label = np.argmax(np.array(probs), axis = 1)    ## taking out the arguments of max values\n",
    "    cost = -np.log(p)    ## (Only data loss. No regularised loss)\n",
    "    \n",
    "    return p,cost,probs,prob_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing weights and bias\n",
    "\n",
    "W1 = 0.1*np.random.rand(3,3,3,1)\n",
    "W2 = 0.1*np.random.rand(3,3,3,3)\n",
    "\n",
    "b1 = 0.1*np.random.rand(3,1)\n",
    "b2 = 0.1*np.random.rand(3,1)\n",
    "\n",
    "## Initializing weights and bias for fully connected layer\n",
    "\n",
    "theta3 = 0.1*np.random.rand(507,10)\n",
    "bias3 = 0.1*np.random.rand(1,10)\n",
    "\n",
    "## Normalizing input data\n",
    "x_arr -= int(np.mean(x_arr))\n",
    "x_arr = x_arr.astype(float)\n",
    "x_arr /= int(np.std(x_arr))\n",
    "\n",
    "train_data = np.hstack((x_arr,np.array(y_train)))     ## horizontally stacking the features and labels \n",
    "t = train_data[0:320]      ## training the model on only 300 examples images due to heavy computation issue\n",
    "\n",
    "## Normalizing cross-validation data\n",
    "x_cv_arr -= int(np.mean(x_cv_arr))\n",
    "x_cv_arr = x_cv_arr.astype(float)\n",
    "x_cv_arr /= int(np.std(x_cv_arr))\n",
    "\n",
    "cv_data = np.hstack((x_cv_arr,np.array(y_cv)))\n",
    "test_data = x_cv_arr[0:100]    ## cross-validating the model on only 100 examples images due to heavy computation issue   \n",
    "Y_cv = np.array(y_cv)[0:100]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "## Assigning hyperparameter values\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "num_images = len(t)   ##Number of the input training examples\n",
    "w = 28\n",
    "l = 1\n",
    "l1 = len(W1)    ## no. opf filters in W1 and W2 \n",
    "l2 = len(W2)    \n",
    "f = len(W1[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_init(train_data,W1,W2,b1,b2,theta3,bias3):\n",
    "    cost = []\n",
    "    accuracy = []\n",
    "    for epoch in range(0, num_epochs):\n",
    "        batches = [train_data[k:k + batch_size] for k in xrange(0, len(train_data), batch_size)]\n",
    "        x=0\n",
    "        i = 1\n",
    "        for batch in batches:\n",
    "            \n",
    "            output = optimizer(batch,learning_rate,W1,W2,b1,b2,theta3,bias3)\n",
    "            [W1, W2, b1, b2, theta3, bias3, batch_cost,acc_,batch_acc] = output\n",
    "            \n",
    "            #epoch_acc = round(np.sum(accuracy[epoch*num_images/batch_size:])/(x+1),2)\n",
    "            \n",
    "            cost.append(batch_cost)\n",
    "            accuracy.append(batch_acc)\n",
    "\n",
    "            print 'ep:%d, batch_num = %f, batch_cost = %f, batch_acc = %f' %(epoch,i,batch_cost,batch_acc) \n",
    "            i+=1\n",
    "    plt.subplot(121),plt.plot(range(int(num_epochs*num_images/batch_size)), cost),plt.title('Batch_cost')        \n",
    "#     plt.plot(range(int(epoch*num_images/batch_size)), cost)\n",
    "    plt.ylabel('Batch Training loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.subplot(122),plt.plot(range(int(num_epochs*num_images/batch_size)), accuracy),plt.title('Batch_accuracy')       \n",
    "    plt.ylabel('Batch Training Accuracy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.show()\n",
    "    \n",
    "    return W1,W2,b1,b2,theta3,bias3,cost,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:0, batch_num = 1.000000, batch_cost = 2.325595, batch_acc = 0.000000\n",
      "ep:0, batch_num = 2.000000, batch_cost = 4.688427, batch_acc = 0.000000\n",
      "ep:0, batch_num = 3.000000, batch_cost = 7.565187, batch_acc = 0.000000\n",
      "ep:0, batch_num = 4.000000, batch_cost = 7.007376, batch_acc = 0.000000\n",
      "ep:0, batch_num = 5.000000, batch_cost = 8.327974, batch_acc = 0.000000\n",
      "ep:0, batch_num = 6.000000, batch_cost = 10.903655, batch_acc = 0.000000\n",
      "ep:0, batch_num = 7.000000, batch_cost = 10.966918, batch_acc = 0.000000\n",
      "ep:0, batch_num = 8.000000, batch_cost = 10.743305, batch_acc = 0.000000\n",
      "ep:0, batch_num = 9.000000, batch_cost = 8.255146, batch_acc = 0.000000\n",
      "ep:0, batch_num = 10.000000, batch_cost = 6.224338, batch_acc = 0.000000\n",
      "ep:1, batch_num = 1.000000, batch_cost = 5.299236, batch_acc = 0.000000\n",
      "ep:1, batch_num = 2.000000, batch_cost = 3.972283, batch_acc = 0.000000\n",
      "ep:1, batch_num = 3.000000, batch_cost = 4.000282, batch_acc = 0.000000\n",
      "ep:1, batch_num = 4.000000, batch_cost = 3.856511, batch_acc = 0.000000\n",
      "ep:1, batch_num = 5.000000, batch_cost = 2.002864, batch_acc = 0.000000\n",
      "ep:1, batch_num = 6.000000, batch_cost = 2.776552, batch_acc = 0.000000\n",
      "ep:1, batch_num = 7.000000, batch_cost = 4.622403, batch_acc = 0.000000\n",
      "ep:1, batch_num = 8.000000, batch_cost = 4.416952, batch_acc = 0.000000\n",
      "ep:1, batch_num = 9.000000, batch_cost = 4.850279, batch_acc = 0.000000\n",
      "ep:1, batch_num = 10.000000, batch_cost = 3.121437, batch_acc = 0.000000\n",
      "ep:2, batch_num = 1.000000, batch_cost = 2.870579, batch_acc = 0.000000\n",
      "ep:2, batch_num = 2.000000, batch_cost = 2.115777, batch_acc = 0.000000\n",
      "ep:2, batch_num = 3.000000, batch_cost = 1.298535, batch_acc = 0.000000\n",
      "ep:2, batch_num = 4.000000, batch_cost = 1.514542, batch_acc = 0.000000\n",
      "ep:2, batch_num = 5.000000, batch_cost = 3.716667, batch_acc = 0.000000\n",
      "ep:2, batch_num = 6.000000, batch_cost = 3.159895, batch_acc = 0.000000\n",
      "ep:2, batch_num = 7.000000, batch_cost = 2.016038, batch_acc = 0.000000\n",
      "ep:2, batch_num = 8.000000, batch_cost = 1.287782, batch_acc = 0.000000\n",
      "ep:2, batch_num = 9.000000, batch_cost = 0.812266, batch_acc = 0.000000\n",
      "ep:2, batch_num = 10.000000, batch_cost = 0.848050, batch_acc = 0.000000\n",
      "ep:3, batch_num = 1.000000, batch_cost = 0.697104, batch_acc = 0.000000\n",
      "ep:3, batch_num = 2.000000, batch_cost = 0.907871, batch_acc = 0.000000\n",
      "ep:3, batch_num = 3.000000, batch_cost = 0.621434, batch_acc = 0.000000\n",
      "ep:3, batch_num = 4.000000, batch_cost = 0.987856, batch_acc = 0.000000\n",
      "ep:3, batch_num = 5.000000, batch_cost = 1.563672, batch_acc = 0.000000\n",
      "ep:3, batch_num = 6.000000, batch_cost = 2.246558, batch_acc = 0.000000\n",
      "ep:3, batch_num = 7.000000, batch_cost = 1.214621, batch_acc = 0.000000\n",
      "ep:3, batch_num = 8.000000, batch_cost = 0.737724, batch_acc = 0.000000\n",
      "ep:3, batch_num = 9.000000, batch_cost = 0.228571, batch_acc = 0.000000\n",
      "ep:3, batch_num = 10.000000, batch_cost = 0.565896, batch_acc = 0.000000\n",
      "ep:4, batch_num = 1.000000, batch_cost = 0.530557, batch_acc = 0.000000\n",
      "ep:4, batch_num = 2.000000, batch_cost = 0.543621, batch_acc = 0.000000\n",
      "ep:4, batch_num = 3.000000, batch_cost = 0.179207, batch_acc = 0.000000\n",
      "ep:4, batch_num = 4.000000, batch_cost = 0.633063, batch_acc = 0.000000\n",
      "ep:4, batch_num = 5.000000, batch_cost = 0.657703, batch_acc = 0.000000\n",
      "ep:4, batch_num = 6.000000, batch_cost = 0.676912, batch_acc = 0.000000\n",
      "ep:4, batch_num = 7.000000, batch_cost = 0.731974, batch_acc = 0.000000\n",
      "ep:4, batch_num = 8.000000, batch_cost = 0.890274, batch_acc = 0.000000\n",
      "ep:4, batch_num = 9.000000, batch_cost = 0.301638, batch_acc = 0.000000\n",
      "ep:4, batch_num = 10.000000, batch_cost = 0.478465, batch_acc = 0.000000\n",
      "ep:5, batch_num = 1.000000, batch_cost = 0.427354, batch_acc = 0.000000\n",
      "ep:5, batch_num = 2.000000, batch_cost = 0.399192, batch_acc = 0.000000\n",
      "ep:5, batch_num = 3.000000, batch_cost = 0.304280, batch_acc = 0.000000\n",
      "ep:5, batch_num = 4.000000, batch_cost = 0.721482, batch_acc = 0.000000\n",
      "ep:5, batch_num = 5.000000, batch_cost = 0.945470, batch_acc = 0.000000\n",
      "ep:5, batch_num = 6.000000, batch_cost = 1.562327, batch_acc = 0.000000\n",
      "ep:5, batch_num = 7.000000, batch_cost = 0.991570, batch_acc = 0.000000\n",
      "ep:5, batch_num = 8.000000, batch_cost = 0.622063, batch_acc = 0.000000\n",
      "ep:5, batch_num = 9.000000, batch_cost = 0.222505, batch_acc = 0.000000\n",
      "ep:5, batch_num = 10.000000, batch_cost = 0.490250, batch_acc = 0.000000\n",
      "ep:6, batch_num = 1.000000, batch_cost = 0.433424, batch_acc = 0.000000\n",
      "ep:6, batch_num = 2.000000, batch_cost = 0.428671, batch_acc = 0.000000\n",
      "ep:6, batch_num = 3.000000, batch_cost = 0.159525, batch_acc = 0.000000\n",
      "ep:6, batch_num = 4.000000, batch_cost = 0.459163, batch_acc = 0.000000\n",
      "ep:6, batch_num = 5.000000, batch_cost = 0.426780, batch_acc = 0.000000\n",
      "ep:6, batch_num = 6.000000, batch_cost = 0.179246, batch_acc = 0.000000\n",
      "ep:6, batch_num = 7.000000, batch_cost = 0.247348, batch_acc = 0.000000\n",
      "ep:6, batch_num = 8.000000, batch_cost = 0.572378, batch_acc = 0.000000\n",
      "ep:6, batch_num = 9.000000, batch_cost = 0.126975, batch_acc = 0.000000\n",
      "ep:6, batch_num = 10.000000, batch_cost = 0.396307, batch_acc = 0.000000\n",
      "ep:7, batch_num = 1.000000, batch_cost = 0.368647, batch_acc = 0.000000\n",
      "ep:7, batch_num = 2.000000, batch_cost = 0.461274, batch_acc = 0.000000\n",
      "ep:7, batch_num = 3.000000, batch_cost = 0.164846, batch_acc = 0.000000\n",
      "ep:7, batch_num = 4.000000, batch_cost = 0.427392, batch_acc = 0.000000\n",
      "ep:7, batch_num = 5.000000, batch_cost = 0.400876, batch_acc = 0.000000\n",
      "ep:7, batch_num = 6.000000, batch_cost = 0.164396, batch_acc = 0.000000\n",
      "ep:7, batch_num = 7.000000, batch_cost = 0.210940, batch_acc = 0.000000\n",
      "ep:7, batch_num = 8.000000, batch_cost = 0.456734, batch_acc = 0.000000\n",
      "ep:7, batch_num = 9.000000, batch_cost = 0.122944, batch_acc = 0.000000\n",
      "ep:7, batch_num = 10.000000, batch_cost = 0.356119, batch_acc = 0.000000\n",
      "ep:8, batch_num = 1.000000, batch_cost = 0.300625, batch_acc = 0.000000\n",
      "ep:8, batch_num = 2.000000, batch_cost = 0.316513, batch_acc = 0.000000\n",
      "ep:8, batch_num = 3.000000, batch_cost = 0.276686, batch_acc = 0.000000\n",
      "ep:8, batch_num = 4.000000, batch_cost = 0.556999, batch_acc = 0.000000\n",
      "ep:8, batch_num = 5.000000, batch_cost = 0.691575, batch_acc = 0.000000\n",
      "ep:8, batch_num = 6.000000, batch_cost = 0.909399, batch_acc = 0.000000\n",
      "ep:8, batch_num = 7.000000, batch_cost = 0.597837, batch_acc = 0.000000\n",
      "ep:8, batch_num = 8.000000, batch_cost = 0.500298, batch_acc = 0.000000\n",
      "ep:8, batch_num = 9.000000, batch_cost = 0.208076, batch_acc = 0.000000\n",
      "ep:8, batch_num = 10.000000, batch_cost = 0.343553, batch_acc = 0.000000\n",
      "ep:9, batch_num = 1.000000, batch_cost = 0.278985, batch_acc = 0.000000\n",
      "ep:9, batch_num = 2.000000, batch_cost = 0.344818, batch_acc = 0.000000\n",
      "ep:9, batch_num = 3.000000, batch_cost = 0.315085, batch_acc = 0.000000\n",
      "ep:9, batch_num = 4.000000, batch_cost = 0.471514, batch_acc = 0.000000\n",
      "ep:9, batch_num = 5.000000, batch_cost = 0.408862, batch_acc = 0.000000\n",
      "ep:9, batch_num = 6.000000, batch_cost = 0.165726, batch_acc = 0.000000\n",
      "ep:9, batch_num = 7.000000, batch_cost = 0.206657, batch_acc = 0.000000\n",
      "ep:9, batch_num = 8.000000, batch_cost = 0.558677, batch_acc = 0.000000\n",
      "ep:9, batch_num = 9.000000, batch_cost = 0.257224, batch_acc = 0.000000\n",
      "ep:9, batch_num = 10.000000, batch_cost = 0.566686, batch_acc = 0.000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-88dfd3ec8414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW1_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta3_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias3_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-630fdb10535f>\u001b[0m in \u001b[0;36mmain_init\u001b[0;34m(train_data, W1, W2, b1, b2, theta3, bias3)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m'ep:%d, batch_num = %f, batch_cost = %f, batch_acc = %f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch_cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#     plt.plot(range(int(epoch*num_images/batch_size)), cost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "W1_t,W2_t,b1_t,b2_t,theta3_t,bias3_t,cost_t,accuracy_t = main_init(t,W1,W2,b1,b2,theta3,bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
